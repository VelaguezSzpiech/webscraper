d6b26db3ecb27c19ac3d36dfb91c83426b964ed0342419245527bf793f2fed76
Main Page: On this pageGetting StartedAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.Main Featuresâ€‹AutoGen enables building next-gen LLM applications based onmulti-agent conversationswith minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.It supportsdiverse conversation patternsfor complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,
the number of agents, and agent conversation topology.It provides a collection of working systems with different complexities. These systems span awide range of applicationsfrom various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.AutoGen providesenhanced LLM inference. It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.AutoGen is powered by collaborativeresearch studiesfrom Microsoft, Penn State University, and University of Washington.Quickstartâ€‹Install from pip:pip install pyautogen. Find more options inInstallation.
Forcode execution, we strongly recommend installing the python docker package, and using docker.Multi-Agent Conversation Frameworkâ€‹Autogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools and human.
By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. Forexample,fromautogenimportAssistantAgent,UserProxyAgent,config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list=config_list_from_json(env_or_file="OAI_CONFIG_LIST")assistant=AssistantAgent("assistant",llm_config={"config_list":config_list})user_proxy=UserProxyAgent("user_proxy",code_execution_config={"work_dir":"coding"})user_proxy.initiate_chat(assistant,message="Plot a chart of NVDA and TESLA stock price change YTD.")# This initiates an automated chat between the two agents to solve the taskCopyThe figure below shows an example conversation flow with AutoGen.Code examples.Documentation.Enhanced LLM Inferencesâ€‹Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.# perform tuning for openai<1config,analysis=autogen.Completion.tune(data=tune_data,metric="success",mode="max",eval_func=eval_func,inference_budget=0.05,optimization_budget=3,num_samples=-1,)# perform inference for a test instanceresponse=autogen.Completion.create(context=test_instance,**config)CopyCode examples.Documentation.Where to Go Next ?â€‹Understand the use cases formulti-agent conversationandenhanced LLM inference.Findcode examples.ReadSDK.Learn aboutresearcharound AutoGen.RoadmapChat onDiscord.Follow onTwitter.If you like our project, please give it astaron GitHub. If you are interested in contributing, please readContributor's Guide.Edit this pageNextInstallationÂ»Main FeaturesQuickstartWhere to Go Next ?
Other Pages: ['Getting Started | AutoGenSkip to main contentAutoGenDocsSDKBlogFAQGitHubðŸŒœðŸŒžctrlKAutoGenðŸŒœðŸŒžDocsSDKBlogFAQGitHubâ† Back to main menuGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageGetting StartedAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.Main Features\u200bAutoGen enables building next-gen LLM applications based onmulti-agent conversationswith minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.It supportsdiverse conversation patternsfor complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\nthe number of agents, and agent conversation topology.It provides a collection of working systems with different complexities. These systems span awide range of applicationsfrom various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.AutoGen providesenhanced LLM inference. It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.AutoGen is powered by collaborativeresearch studiesfrom Microsoft, Penn State University, and University of Washington.Quickstart\u200bInstall from pip:pip install pyautogen. Find more options inInstallation.\nForcode execution, we strongly recommend installing the python docker package, and using docker.Multi-Agent Conversation Framework\u200bAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools and human.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. Forexample,fromautogenimportAssistantAgent,UserProxyAgent,config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list=config_list_from_json(env_or_file="OAI_CONFIG_LIST")assistant=AssistantAgent("assistant",llm_config={"config_list":config_list})user_proxy=UserProxyAgent("user_proxy",code_execution_config={"work_dir":"coding"})user_proxy.initiate_chat(assistant,message="Plot a chart of NVDA and TESLA stock price change YTD.")# This initiates an automated chat between the two agents to solve the taskCopyThe figure below shows an example conversation flow with AutoGen.Code examples.Documentation.Enhanced LLM Inferences\u200bAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.# perform tuning for openai<1config,analysis=autogen.Completion.tune(data=tune_data,metric="success",mode="max",eval_func=eval_func,inference_budget=0.05,optimization_budget=3,num_samples=-1,)# perform inference for a test instanceresponse=autogen.Completion.create(context=test_instance,**config)CopyCode examples.Documentation.Where to Go Next ?\u200bUnderstand the use cases formulti-agent conversationandenhanced LLM inference.Findcode examples.ReadSDK.Learn aboutresearcharound AutoGen.RoadmapChat onDiscord.Follow onTwitter.If you like our project, please give it astaron GitHub. If you are interested in contributing, please readContributor\'s Guide.Edit this pageNextInstallationÂ»CommunityDiscordTwitterCopyright Â© 2023 AutoGen Authors.', 'Installation | AutoGenSkip to main contentAutoGenDocsSDKBlogFAQGitHubðŸŒœðŸŒžctrlKAutoGenðŸŒœðŸŒžDocsSDKBlogFAQGitHubâ† Back to main menuGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageInstallationSetup Virtual Environment\u200bWhen not using a docker container, we recommend using a virtual environment to install AutoGen. This will ensure that the dependencies for AutoGen are isolated from the rest of your system.Option 1: venv\u200bYou can create a virtual environment withvenvas below:python3 -m venv pyautogensourcepyautogen/bin/activateCopyThe following command will deactivate the currentvenvenvironment:deactivateCopyOption 2: conda\u200bAnother option is withConda, Conda works better at solving dependency conflicts than pip. You can install it by followingthis doc,\nand then create a virtual environment as below:conda create -n pyautogenpython=3.10# python 3.10 is recommended as it\'s stable and not too oldconda activate pyautogenCopyThe following command will deactivate the currentcondaenvironment:conda deactivateCopyNow, you\'re ready to install AutoGen in the virtual environment you\'ve just created.Python\u200bAutoGen requiresPython version >= 3.8, < 3.12. It can be installed from pip:pipinstallpyautogenCopypyautogen<0.2requiresopenai<1. Starting from pyautogen v0.2,openai>=1is required.Migration guide to v0.2\u200bopenai v1 is a total rewrite of the library with many breaking changes. For example, the inference requires instantiating a client, instead of using a global class method.\nTherefore, some changes are required for users ofpyautogen<0.2.api_base->base_url,request_timeout->timeoutinllm_configandconfig_list.max_retry_periodandretry_wait_timeare deprecated.max_retriescan be set for each client.MathChat, TeachableAgent are unsupported until they are tested in future release.autogen.Completionandautogen.ChatCompletionare deprecated. The essential functionalities are moved toautogen.OpenAIWrapper:fromautogenimportOpenAIWrapperclient=OpenAIWrapper(config_list=config_list)response=client.create(messages=[{"role":"user","content":"2+2="}])print(client.extract_text_or_function_call(response))CopyInference parameter tuning and inference logging features are currently unavailable inOpenAIWrapper. Logging will be added in a future release.\nInference parameter tuning can be done viaflaml.tune.seedin autogen is renamed intocache_seedto accommodate the newly addedseedparam in openai chat completion api.use_cacheis removed as a kwarg inOpenAIWrapper.create()for being automatically decided bycache_seed: int | None. The difference between autogen\'scache_seedand openai\'sseedis that:autogen uses local disk cache to guarantee the exactly same output is produced for the same input and when cache is hit, no openai api call will be made.openai\'sseedis a best-effort deterministic sampling with no guarantee of determinism. When using openai\'sseedwithcache_seedset to None, even for the same input, an openai api call will be made and there is no guarantee for getting exactly the same output.Optional Dependencies\u200bdocker\u200bFor the best user experience and seamless code execution, we highly recommend using Docker with AutoGen. Docker is a containerization platform that simplifies the setup and execution of your code. Developing in a docker container, such as GitHub Codespace, also makes the development convenient.When running AutoGen out of a docker container, to use docker for code execution, you also need to install the python packagedocker:pipinstalldockerCopyblendsearch\u200bpyautogen<0.2offers a cost-effective hyperparameter optimization techniqueEcoOptiGenfor tuning Large Language Models. Please install with the[blendsearch]option to use it.pipinstall"pyautogen[blendsearch]<0.2"CopyExample notebooks:Optimize for Code GenerationOptimize for Mathretrievechat\u200bpyautogen<0.2supports retrieval-augmented generation tasks such as question answering and code generation with RAG agents. Please install with the[retrievechat]option to use it.pipinstall"pyautogen[retrievechat]<0.2"CopyRetrieveChat can handle various types of documents. By default, it can process\nplain text and PDF files, including formats such as \'txt\', \'json\', \'csv\', \'tsv\',\n\'md\', \'html\', \'htm\', \'rtf\', \'rst\', \'jsonl\', \'log\', \'xml\', \'yaml\', \'yml\' and \'pdf\'.\nIf you installunstructured(pip install "unstructured[all-docs]"), additional document types such as \'docx\',\n\'doc\', \'odt\', \'pptx\', \'ppt\', \'xlsx\', \'eml\', \'msg\', \'epub\' will also be supported.You can find a list of all supported document types by usingautogen.retrieve_utils.TEXT_FORMATS.Example notebooks:Automated Code Generation and Question Answering with Retrieval Augmented AgentsGroup Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented AgentsLarge Multimodal Model (LMM) Agents\u200bWe offered Multimodal Conversable Agent and LLaVA Agent. Please install with the[lmm]option to use it.pipinstall"pyautogen[lmm]"CopyExample notebooks:LLaVA Agentmathchat\u200bpyautogen<0.2offers an experimental agent for math problem solving. Please install with the[mathchat]option to use it.pipinstall"pyautogen[mathchat]<0.2"CopyExample notebooks:Using MathChat to Solve Math ProblemsEdit this pagePreviousÂ«Getting StartedNextMulti-agent Conversation FrameworkÂ»CommunityDiscordTwitterCopyright Â© 2023 AutoGen Authors.', 'Getting Started | AutoGenSkip to main contentAutoGenDocsSDKBlogFAQGitHubðŸŒœðŸŒžctrlKAutoGenðŸŒœðŸŒžDocsSDKBlogFAQGitHubâ† Back to main menuGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageGetting StartedAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.Main Features\u200bAutoGen enables building next-gen LLM applications based onmulti-agent conversationswith minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.It supportsdiverse conversation patternsfor complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\nthe number of agents, and agent conversation topology.It provides a collection of working systems with different complexities. These systems span awide range of applicationsfrom various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.AutoGen providesenhanced LLM inference. It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.AutoGen is powered by collaborativeresearch studiesfrom Microsoft, Penn State University, and University of Washington.Quickstart\u200bInstall from pip:pip install pyautogen. Find more options inInstallation.\nForcode execution, we strongly recommend installing the python docker package, and using docker.Multi-Agent Conversation Framework\u200bAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools and human.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. Forexample,fromautogenimportAssistantAgent,UserProxyAgent,config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list=config_list_from_json(env_or_file="OAI_CONFIG_LIST")assistant=AssistantAgent("assistant",llm_config={"config_list":config_list})user_proxy=UserProxyAgent("user_proxy",code_execution_config={"work_dir":"coding"})user_proxy.initiate_chat(assistant,message="Plot a chart of NVDA and TESLA stock price change YTD.")# This initiates an automated chat between the two agents to solve the taskCopyThe figure below shows an example conversation flow with AutoGen.Code examples.Documentation.Enhanced LLM Inferences\u200bAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.# perform tuning for openai<1config,analysis=autogen.Completion.tune(data=tune_data,metric="success",mode="max",eval_func=eval_func,inference_budget=0.05,optimization_budget=3,num_samples=-1,)# perform inference for a test instanceresponse=autogen.Completion.create(context=test_instance,**config)CopyCode examples.Documentation.Where to Go Next ?\u200bUnderstand the use cases formulti-agent conversationandenhanced LLM inference.Findcode examples.ReadSDK.Learn aboutresearcharound AutoGen.RoadmapChat onDiscord.Follow onTwitter.If you like our project, please give it astaron GitHub. If you are interested in contributing, please readContributor\'s Guide.Edit this pageNextInstallationÂ»CommunityDiscordTwitterCopyright Â© 2023 AutoGen Authors.', 'Getting Started | AutoGenSkip to main contentAutoGenDocsSDKBlogFAQGitHubðŸŒœðŸŒžctrlKAutoGenðŸŒœðŸŒžDocsSDKBlogFAQGitHubâ† Back to main menuGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageGetting StartedAutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.Main Features\u200bAutoGen enables building next-gen LLM applications based onmulti-agent conversationswith minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.It supportsdiverse conversation patternsfor complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\nthe number of agents, and agent conversation topology.It provides a collection of working systems with different complexities. These systems span awide range of applicationsfrom various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.AutoGen providesenhanced LLM inference. It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.AutoGen is powered by collaborativeresearch studiesfrom Microsoft, Penn State University, and University of Washington.Quickstart\u200bInstall from pip:pip install pyautogen. Find more options inInstallation.\nForcode execution, we strongly recommend installing the python docker package, and using docker.Multi-Agent Conversation Framework\u200bAutogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools and human.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. Forexample,fromautogenimportAssistantAgent,UserProxyAgent,config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list=config_list_from_json(env_or_file="OAI_CONFIG_LIST")assistant=AssistantAgent("assistant",llm_config={"config_list":config_list})user_proxy=UserProxyAgent("user_proxy",code_execution_config={"work_dir":"coding"})user_proxy.initiate_chat(assistant,message="Plot a chart of NVDA and TESLA stock price change YTD.")# This initiates an automated chat between the two agents to solve the taskCopyThe figure below shows an example conversation flow with AutoGen.Code examples.Documentation.Enhanced LLM Inferences\u200bAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.# perform tuning for openai<1config,analysis=autogen.Completion.tune(data=tune_data,metric="success",mode="max",eval_func=eval_func,inference_budget=0.05,optimization_budget=3,num_samples=-1,)# perform inference for a test instanceresponse=autogen.Completion.create(context=test_instance,**config)CopyCode examples.Documentation.Where to Go Next ?\u200bUnderstand the use cases formulti-agent conversationandenhanced LLM inference.Findcode examples.ReadSDK.Learn aboutresearcharound AutoGen.RoadmapChat onDiscord.Follow onTwitter.If you like our project, please give it astaron GitHub. If you are interested in contributing, please readContributor\'s Guide.Edit this pageNextInstallationÂ»CommunityDiscordTwitterCopyright Â© 2023 AutoGen Authors.', 'Contributing | AutoGenSkip to main contentAutoGenDocsSDKBlogFAQGitHubðŸŒœðŸŒžctrlKAutoGenðŸŒœðŸŒžDocsSDKBlogFAQGitHubâ† Back to main menuGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageContributingThis project welcomes and encourages all forms of contributions, including but not limited to:Pushing patches.Code review of pull requests.Documentation, examples and test cases.Readability improvement, e.g., improvement on docstr and comments.Community participation inissues,discussions,discord, andtwitter.Tutorials, blog posts, talks that promote the project.Sharing application scenarios and/or related research.Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visithttps://cla.opensource.microsoft.com.If you are new to GitHubhereis a detailed help source on getting involved with development on GitHub.When you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.This project has adopted theMicrosoft Open Source Code of Conduct.\nFor more information see theCode of Conduct FAQor\ncontactopencode@microsoft.comwith any additional questions or comments.How to make a good bug report\u200bWhen you submit an issue toGitHub, please do your best to\nfollow these guidelines! This will make it a lot easier to provide you with good\nfeedback:The ideal bug report contains a short reproducible code snippet. This way\nanyone can try to reproduce the bug easily (seethisfor more details). If your snippet is\nlonger than around 50 lines, please link to agistor a GitHub repo.If an exception is raised, pleaseprovide the full traceback.Please include youroperating system type and version number, as well as\nyourPython, autogen, scikit-learn versions. The version of autogen\ncan be found by running the following code snippet:importautogenprint(autogen.__version__)CopyPlease ensure allcode snippets and error messages are formatted in\nappropriate code blocks.  SeeCreating and highlighting code blocksfor more details.Becoming a Reviewer\u200bThere is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors. If you are willing to become a reviewer, you are welcome to let us know on discord.Guidance for Maintainers\u200bGeneral\u200bBe a member of the community and treat everyone as a member. Be inclusive.Help each other and encourage mutual help.Actively post and respond.Keep open communication.Pull Requests\u200bFor new PR, decide whether to close without review. If not, find the right reviewers. The default reviewer is microsoft/autogen. Ask users who can benefit from the PR to review it.For old PR, check the blocker: reviewer or PR creator. Try to unblock. Get additional help when needed.When requesting changes, make sure you can check back in time because it blocks merging.Make sure all the checks are passed.For changes that require running OpenAI tests, make sure the OpenAI tests pass too. Running these tests requires approval.In general, suggest small PRs instead of a giant PR.For documentation change, request snapshot of the compiled website, or compile by yourself to verify the format.For new contributors who have not signed the contributing agreement, remind them to sign before reviewing.For multiple PRs which may have conflict, coordinate them to figure out the right order.Pay special attention to:Breaking changes. Donâ€™t make breaking changes unless necessary. Donâ€™t merge to main until enough headsup is provided and a new release is ready.Test coverage decrease.Changes that may cause performance degradation. Do regression test when test suites are available.Discouragechange to the core librarywhen there is an alternative.Issues and Discussions\u200bFor new issues, write a reply, apply a label if relevant. Ask on discord when necessary. For roadmap issues, add to the roadmap project and encourage community discussion. Mention relevant experts when necessary.For old issues, provide an update or close. Ask on discord when necessary. Encourage PR creation when relevant.Use â€œgood first issueâ€ for easy fix suitable for first-time contributors.Use â€œtask listâ€ for issues that require multiple PRs.For discussions, create an issue when relevant. Discuss on discord when appropriate.Developing\u200bSetup\u200bgitclone https://github.com/microsoft/autogen.gitpipinstall-e autogenCopyDocker\u200bWe provide a simpleDockerfile.dockerbuild https://github.com/microsoft/autogen.git#main -t autogen-devdockerrun -it autogen-devCopyDevelop in Remote Container\u200bIf you use vscode, you can open the autogen folder in aContainer.\nWe have provided the configuration indevcontainer. They can be used in GitHub codespace too. Developing AutoGen in dev containers is recommended.Pre-commit\u200bRunpre-commit installto install pre-commit into your git hooks. Before you commit, runpre-commit runto check if you meet the pre-commit requirements. If you use Windows (without WSL) and can\'t commit after installing pre-commit, you can runpre-commit uninstallto uninstall the hook. In WSL or Linux this is supposed to work.Write tests\u200bTests are automatically run via GitHub actions. There are two workflows:build.ymlopenai.ymlThe first workflow is required to pass for all PRs. The second workflow is required for changes that affect the openai tests. The second workflow requires approval to run. When writing tests that require openai, please usepytest.mark.skipifto make them run in one python version only when openai is installed. If additional dependency for this test is required, install the dependency in the corresponding python version inopenai.yml.Coverage\u200bAny code you commit should not decrease coverage. To run all unit tests, install the[test]option:pipinstall-e."[test]"coverage run -m pytesttestCopyThen you can see the coverage report bycoverage report -morcoverage html.Documentation\u200bTo build and test documentation locally, installNode.js. For example,nvminstall--ltsCopyThen:npm install --global yarn  # skip if you use the dev container we providedpip install pydoc-markdown  # skip if you use the dev container we providedcd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownyarn startCopyThe last command starts a local development server and opens up a browser window.\nMost changes are reflected live without having to restart the server.Note:\nsome tips in this guide are based off the contributor guide fromflaml.Edit this pagePreviousÂ«Tune GPT ModelsNextResearchÂ»CommunityDiscordTwitterCopyright Â© 2023 AutoGen Authors.', "Research | AutoGenSkip to main contentAutoGenDocsSDKBlogFAQGitHubðŸŒœðŸŒžctrlKAutoGenðŸŒœðŸŒžDocsSDKBlogFAQGitHubâ† Back to main menuGetting StartedInstallationUse CasesExamplesContributingResearchResearchFor technical details, please check our technical report and research publications.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.@inproceedings{wu2023autogen,title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},year={2023},eprint={2308.08155},archivePrefix={arXiv},primaryClass={cs.AI}}CopyCost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. AutoML'23.@inproceedings{wang2023EcoOptiGen,title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},year={2023},booktitle={AutoML'23},}CopyAn Empirical Study on Challenging Math Problem Solving with GPT-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).@inproceedings{wu2023empirical,title={An Empirical Study on Challenging Math Problem Solving with GPT-4},author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},year={2023},booktitle={ArXiv preprint arXiv:2306.01337},}CopyEcoAssistant: Using LLM Assistant More Affordably and Accurately. Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang. ArXiv preprint arXiv:2310.03046 (2023).@inproceedings{zhang2023ecoassistant,title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},year={2023},booktitle={ArXiv preprint arXiv:2310.03046},}CopyEdit this pagePreviousÂ«ContributingCommunityDiscordTwitterCopyright Â© 2023 AutoGen Authors."]